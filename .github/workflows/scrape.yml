name: Daily Sahi Buzz Scraper

on:
  schedule:
    - cron: '*/1 * * * *'  # 4:00 PM UTC daily
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libgbm-dev \
            wget \
            fonts-liberation \
            libasound2 \
            libatk-bridge2.0-0 \
            libatk1.0-0 \
            libatspi2.0-0 \
            libcups2 \
            libdbus-1-3 \
            libdrm2 \
            libgtk-3-0 \
            libnspr4 \
            libnss3 \
            libxcomposite1 \
            libxdamage1 \
            libxfixes3 \
            libxkbcommon0 \
            libxrandr2 \
            xdg-utils

      - name: Install Chrome
        run: |
          wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
          sudo apt install -y ./google-chrome-stable_current_amd64.deb
          rm google-chrome-stable_current_amd64.deb
          echo "CHROME_PATH=$(which google-chrome)" >> $GITHUB_ENV

      - name: Install puppeteer-core
        run: npm install puppeteer-core

      - name: Run scraping script
        run: node scrape.js > output.log 2>&1
        env:
          NODE_OPTIONS: --unhandled-rejections=strict
          PUPPETEER_EXECUTABLE_PATH: ${{ env.CHROME_PATH }}

      - name: Show logs
        run: cat output.log

      - name: Archive results
        uses: actions/upload-artifact@v4
        with:
          name: sahi-buzz-results
          path: |
            output.json
            output.log
